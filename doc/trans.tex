\documentclass[english]{article}

\bibliographystyle{unsrt}
\input{defs.tex}
\input{notation.tex}
\begin{document}

\newcommand\authorstring{
Ian Holmes$^{1,\ast}$ \\
\textbf{1} Department of Bioengineering, University of California, Berkeley, CA, USA \\
$\ast$ E-mail: ihh@berkeley.edu
}

\newcommand\titlestring{Transducer codes for DNA}
\newcommand\shorttitlestring{Transducer codes for DNA}
\markboth{\shorttitlestring}{\shorttitlestring}
\begin{flushleft}
{\Large \textbf{\titlestring} } \\
\authorstring
\end{flushleft}
%\section{Abstract}
%\paragraph{Keywords:}
%\tableofcontents

\section{Introduction}

DNA storage \cite{ChurchEtAl2012,GoldmanEtAl2013}.
Codes \cite{YazdiEtAl2015}


\cite{GoldmanEtAl2013} use a trinary (radix-3) code that avoids repeated nucleotides in the synthesized DNA.
Avoidance of dinucleotide repeats leaves 3 available possibilities at each position, hence the radix-3 code
Their argument is that this protects against most common class of errors in DNA resequencing which occur at homopolymer runs
However, this is just one point on a spectrum of possibilities
Different applications might require different codes
e.g. in a world where synthesis is cheap and sequencing is error-prone, then use error-tolerant codes that may add overhead, but want to keep overhead as low as possible
Or if archiving data for retrieval in distant future, given current market where synthesis remains expensive and sequencing is becoming ever cheaper and more accurate, might be better to pack as much information into the DNA as possible ---but there may still be a need to interleave
 ``reserved words'' into the DNA sequence,
including e.g.
metadata (especially facilitating random access or scaffolded assembly),
checksums,
escape sequences,
barcodes,
start and end markers for files or blocks,
or biologically relevant motifs such as restriction enzyme sites.

We present a modular strategy for designing codes that meet flexible needs
including error-avoidance, error-correction, and demarcated metadata
Core idea of our approach is to convert raw binary data (with filesystem metadata) into a mixed-radix sequence
wherein successive digits may be binary (radix-2), trinary (radix-3) or quaternary (radix-4)
The radix at any given position is effectively specified by the number of available nucleotides at that position
(which may vary due to requirements of avoiding repeats or reserved motifs)
The mixed-radix sequence can then be efficiently converted to a DNA sequence
Error-correcting codes (for example, Hamming codes that introduce parity bits)
can be applied upstream of the conversion from binary to mixed-radix,
while the readout process (errors from which may reintroduce repeats and prohibited motifs)
can be integrated into the decoding model as a step that follows the conversion from mixed-radix to DNA

To combine error-correcting codes, repeat-avoidant nucleotide encoding, and a statistical error model
in a single coherent framework,
we represent the stages of the encoding and decoding process
modularly as finite-state machines known as transducers \cite{MohriPereiraRiley2000,WikipediaTransducers}
Standard algorithms for transducer combination (e.g. transducer composition)
and decoding (Viterbi) can then be used
Previous uses of transducers in bioinformatics:
protein classification \cite{EskinEtAl2000},
phylogenetic indel models \cite{PatenEtAl2008,WestessonEtAlArxiv2012,WestessonEtAl2012},
cancer informatics \cite{SchwarzEtAl2014}

We also use
De Bruijn graphs \cite{DeBruijn1946,PevznerEtAl2001,ZerbinoBirney2008,IqbalEtAl2012}
and relationship to automata with limited context e.g. order-$N$ Markov models


\section{Methods}

\subsection{Weighted finite-state transducers}

Following \cite{MohriPereiraRiley2000}:
Assume a general semiring
$\semiring=(\srset,\srplus,\srtimes,\srplusid,\srtimesid)$
which for our purposes is typically the probability semiring
$(\Re,+,\times,0,1)$
or the tropical semiring
$(\Re_{+} \cup {\infty},\min,+,\infty,0)$.

A weighted finite-state transducer is defined as a tuple
$\trt = (\inalph,\outalph,\states,\transitions,\initstate,\finalstate)$
consisting of an input alphabet $\inalph$,
an output alphabet $\outalph$ (both alphabets being finite sets),
a finite set of states $\states$,
a finite set of transitions
$\transitions \subseteq \states \times (\maybe{\inalph}) \times (\maybe{\outalph}) \times \srset \times \states$,
an initial state $\initstate \in \states$
and a final state $\finalstate \in \states$.

The transducer $\trt$ can be thought of as an edge-labelled directed graph
where each state is a vertex
and each transition
$\trans = (\src[\trans],\inlab[\trans],\outlab[\trans],\weight[\trans],\dest[\trans]) \in \transitions$
is an edge from state $\src[\trans]$ to state $\dest[\trans]$
with input label $\inlab[\trans]$,
output label $\outlab[\trans]$
and weight $\weight[\trans]$.

A path in $\trt$ is a series of transitions that form a path in this graph.
The input sequence and output sequence for a path are the concatenation of (respectively)
the input and output labels of the transitions in the path.
The path weight is the $\srtimes$-product of the transition weights.
A successful path is one that starts in $\initstate$ and ends in $\finalstate$.
The transduction weight for a given input sequence $\inseq \in \inseqs$
and output sequence $\outseq \in \outseqs$
is the $\srplus$-sum of all successful paths
having $\inseq$ as the input sequence
and $\outseq$ as the output sequence.
Thus $\trt$ provides a mapping
$\tfunc{T}:(\kleene{\inalph} \times \kleene{\outalph}) \to \srset$
from sequence-pairs to weights.
We call this mapping $\tfunc{T}$ the transducer function.
For a given pair of sequences $(\inseq,\outseq)$
and a semiring wherein $\srplus$ and $\srtimes$ are amortized-constant resource operations,
it can be evaluated in time $\bigo(|\inseq| \cdot |\outseq| \cdot |\transitions|)$
and memory $\bigo(|\inseq| \cdot |\outseq| \cdot |\states|)$
by dynamic programming,
analogously to the Forward algorithm in the probabilistic semiring
or the Viterbi algorithm in the tropical semiring
\cite{Durbin98}.

A state $\state \in \states$ has past input context $\seqpast$ if every path from $\initstate$ to $\state$ has an input sequence with suffix $\seqpast$,
and future input context $\seqnext$ if every path from $\state$ to $\finalstate$ has an input sequence with prefix $\seqnext$.
The past output context and future output context of a state are defined similarly.

A ``wait state'' is a state whose outgoing transitions all have nonempty input labels.
A ``waiting machine'' is a transducer where all the transitions with nonempty input labels
originate from wait states.
Any transducer $\trt$ with a transducer function $\tfunc{T}$
can be transformed into an equivalent waiting machine
that has the same transducer function $\tfunc{T}$ and
at most $2|\states|$ states and $|\states|+|\transitions|$ transitions
\cite{WestessonEtAlArxiv2012}.

For the figures in this paper,
we adopt the convention that transition labels are annotated as $\inlab{\tt /}\outlab$
(the transition weights for the transducers in the figures are mostly equal to 1, and are not shown).
\figref{HammingTransducer} shows transducers that implement the Hamming(3,1) and Hamming(7,4) error-correcting codes,
\figref{NaiveDNATransducer} shows transducers that convert binary (radix-2) strings into quaternary (radix-4) and DNA,
\figref{DivisionByThree} and \figref{EvenDivision} show transducers that implement long division
(and so are useful for radix conversion),
\figref{Converter} shows more transducers that map between sequences of differing and mixed radix,
and \figref{DNAStore} shows repeat-avoiding transducers that encode mixed-radix sequences,
constructed using the method of \secref{DeBruijnTransducer}.

\subsection{Transducer composition}

Given transducers
 $\trr = (\inalph, \somealph, \tpr{\states}, \tpr{\transitions}, \tpr{\initstate}, \tpr{\finalstate})$ and
 $\trs = (\somealph, \outalph, \tps{\states}, \tps{\transitions}, \tps{\initstate}, \tps{\finalstate})$
where $\trr$'s output alphabet is the same as $\trs$'s input alphabet,
we can readily find a composite transducer
 $\trt = \trr \transcomp \trs = (\inalph, \outalph, \tpt{\states}, \tpt{\transitions}, \tpt{\initstate}, \tpt{\finalstate})$
such that, if $\tfunc{R}$, $\tfunc{S}$ and $\tfunc{T}$ are the corresponding transducer functions,
then
\[
\forall \inseq \in \inseqs, \outseq \in \outseqs:
\quad
\tfunc{T}(\inseq,\outseq) = \srsum_{\someseq \in \someseqs} \tfunc{R}(\inseq,\someseq) \tfunc{S}(\someseq,\outseq)
\]
that is, $\trt$ models the feeding of $\trr$'s output into $\trs$'s input
(and this intermediate sequence is then summed out---i.e. marginalized, if we are in the probabilistic semiring).

Loosely speaking, we can construct $\trt$ using the following recipe:
\begin{itemize}
\item Each $\trt$-state corresponds to a pair of $\trr$- and $\trs$-states,
so $\tpt{\state} = (\tpr{\state}, \tps{\state})$
and $\tpt{\states} \subseteq \tpr{\states} \times \tps{\states}$.
\item The initial $\trt$-state $\tpt{\initstate}=(\tpr{\initstate},\tps{\initstate})$ pairs the initial states of $\trr$ and $\trs$.
\item The final $\trt$-state $\tpt{\finalstate}=(\tpr{\finalstate},\tps{\finalstate})$ pairs the final states of $\trr$ and $\trs$.
\item $\trt$-transitions
$\tpt{\trans} = ((\tpr{\src},\tps{\src}),\inlab,\outlab,\tpt{\weight},(\tpr{\dest},\tps{\dest}))$
represent (summaries of sets of) synchronized pairs of $\trr$-transitions
$\tpr{\trans} = (\tpr{\src},\inlab,\midlab,\tpr{\weight},\tpr{\dest})$
and $\trs$-transitions
$\tps{\trans} = (\tps{\src},\midlab,\outlab,\tps{\weight},\tps{\dest})$
which share the same intermediate label $\midlab$.
The composite transition weight $\tpt{\weight}$ is the product $\tpr{\weight} \srtimes \tps{\weight}$,
or the sum over such products if there are multiple transition-pairs $(\tpr{\trans},\tps{\trans})$
consistent with a given $\tpt{\trans}$
(which, in the probabilistic semiring, is equivalent to marginalizing out $\midlab$).
\item Some additional manipulation is required to synchronize
transitions involving empty input labels (``insertions''),
empty output labels (``deletions''),
or both (``null transitions'').
For example, we can require that $\trr$ and $\trs$ are waiting machines,
or (if they aren't) we can work with their equivalent waiting machines \cite{WestessonEtAlArxiv2012}.
\end{itemize}

This algorithm can be made precise enough to automate;
more detailed workings are given elsewhere \cite{PereiraRiley1996,MohriPereiraRiley2000,Holmes2003,Holmes2007,WestessonEtAlArxiv2012,WestessonEtAl2012}.
The transducers yielded by a fully automated implementation can typically be aggressively optimized by hand
to minimize the number of transitions and/or states,
and hence,
the time and/or memory complexity of dynamic programming algorithms.
A simple example of (manual) transducer composition is shown in \figref{NaiveDNATransducer}.

\subsection{Transducer union and Kleene closure}

% WRITE ME
\figref{DivisionByThree}

\subsection{Transducer concatenation}

Anther operation on transducers that is useful in constructing DNA codes
is concatenation.
Suppose
 $\tra = (\inalph, \outalph, \tpa{\states}, \tpa{\transitions}, \tpa{\initstate}, \tpa{\finalstate})$ and
 $\trb = (\inalph, \outalph, \tpb{\states}, \tpb{\transitions}, \tpb{\initstate}, \tpb{\finalstate})$
are transducers with the same input and output alphabets and disjoint state spaces.
The concatenated transducer $\trt = \tra \transconcat \trb$ can be constructed by
taking the union of the state spaces and adding a null ($\epsilon/\epsilon$) unit weight transition
from $\tpa{\finalstate}$ to $\tpb{\initstate}$.
This models feeding the part of a sequence through $\tra$ and then feeding the second part through $\trb$.
Some simple examples of concatenation are shown in \figref{DivisionByThree}.

\subsection{De Bruijn graphs}

Denote by $\somealph^\kmerlen$
the set of all possible $\kmerlen$-symbol strings
$\sym_1 \sym_2 \ldots \sym_\kmerlen$ over some alphabet $\somealph$.

The $\kmerlen$-dimensional De Bruijn graph over $\somealph$
has vertex set $\somealph^\kmerlen$
and a directed edge $u \to v$ between any two vertices
$u=\sym_1 \sym_2 \ldots \sym_\kmerlen$ and $v=\sym_2 \ldots \sym_\kmerlen \sym_{\kmerlen+1}$
that overlap by $\kmerlen-1$ symbols; this edge is labeled with symbol $\sym_{\kmerlen+1}$ (the last symbol of $v$).
Thus, each vertex has $|\somealph|$ incoming and $|\somealph|$ outgoing edges \cite{DeBruijn1946,PevznerEtAl2001}.
Denote this graph by $\debruijngraph(\somealph^\kmerlen) = (\somealph^\kmerlen,\debruijnedges(\somealph^\kmerlen))$.

\subsection{DNA alphabets and repeats}

Let $\nucalph = \{ \mbox{A}, \mbox{C}, \mbox{G}, \mbox{T} \} $ be the nucleotide alphabet.
Let $\comp{x}$ denote the complement of a nucleotide symbol $x$,
and $\revcomp{\someseq}$ the reverse complement of a nucleotide sequence $\someseq$.
Let $\ntrans{x}$ denote the nucleotide related to $x$ by a transition substitution
(so $\ntrans{\nuca} = \nucg$, $\ntrans{\nucc}=\nuct$, etc.).

A direct tandem repeat of length $k$ is a nucleotide sequence followed by an exact copy of itself, $\someseq \someseq$, where the length of each copy is $\seqlen{\someseq}=k$
(note this includes repeated single nucleotides when $k=1$).
Similarly, a direct inverted repeat of length $k$ is a $k$-nucleotide sequence followed by its reverse complement, $\someseq \revcomp{\someseq}$.
A local inverted repeat of length $k$ and separation $l$ is a $k$-nucleotide sequence, followed an $l$-nucleotide sequence, followed by the reverse complement
of the first sequence; that is, $\someseq \otherseq \revcomp{\someseq}$, where $\seqlen{\someseq}=k$ and $\seqlen{\otherseq}=l$.

\subsection{A transducer for encoding signals as DNA without short repeats or reserved words}
\seclabel{DeBruijnTransducer}

In this section we contruct, in several steps, a transducer $\transcoder$
that accepts a mixed-radix sequence on the input
(that is, every input state either accepts binary symbols $\{ \bit{0}, \bit{1} \}$,
trinary symbols $\{ \trit{0}, \trit{1}, \trit{2} \}$ or
quaternary symbols $\{ \quat{0}, \quat{1}, \quat{2}, \quat{3} \}$)
and outputs a (uniquely decodable) nucleotide sequence that is free of
repeated nucleotides, short tandem repeats, or inverted repeats.
The input sequence may optionally be interleaved with special control digits
$\{ \controlsym{i}:\ 1 \leq i \leq \ncontrols \}$
which force the machine to output predictable, recognizable nucleotide sequences
that can be used to flag metadata, demarcate boundaries,
or to embed biologically functional motifs.

Start with $\debruijngraph(\nucalph^\kmerlen)$, the $\kmerlen$-dimensional De Bruijn graph over $\nucalph$.
Delete all nodes corresponding to sequences that are (or contain substrings which are)
direct tandem repeats of any length,
direct inverted repeats of length $\geq 2$,
or local inverted repeats of length $\geq \invreplen$ and separation $\geq 2$.
Denote the resulting graph by $\norepgraph = (\norepvertices,\norepedges)$.

Let $A \subset \norepvertices$ be a set of vertices to avoid,
let $D \in A$ be a target vertex,
and denote by $\prequels(D,A,N) \subset \norepvertices$
the set of all vertices
from which there exists a length-$N$ path to $D$
that does not pass through any of the vertices in $A$
(although the path is allowed to originate from one of those vertices).
Define $\stepsto(D,A)$ to be the smallest value of $N$ for which $\prequels(D,A,N) = \norepvertices$,
if such a value of $N$ exists; otherwise, let $\stepsto(D,A)$ be $\infty$.
We can find $\prequels(D,A,N)$ from $\prequels(D,A,N-1)$ by recursive backtracking,
and so determine in $k$ steps whether $\stepsto(D,A) \leq k$.

We now allocate $\ncontrols$ ``control words'': $\kmerlen$-mers are reserved for marking metadata boundaries,
including the starts and ends of messages.
In the transducer, these will be unreachable except by specially constructed paths of uniform length.
Specifically, we seek an indexed set of vertices
$\controlset = \{ \controlword_1, \controlword_2 \ldots \controlword_{\ncontrols} \}$
such that $\stepsto(\controlword_n,\controlset) \leq k$ for all $n$
and for some pre-specified value of $k$.
Our implementation finds this list $\controlset$ by brute-force recursive search
(using $k = 2\kmerlen$)
and further attempts to maximize the shortest Hamming distance between any two control words in $\controlset$.
Note that there is a ceiling to the number of control words that may be found
for any given value of $\kmerlen$ (and $\invreplen$),
though this ceiling grows rather rapidly with $\kmerlen$.
In practice we only need a few control words for most purposes.

Having designated some $\kmerlen$-mers as control words via analysis of $\norepgraph$,
we now construct a new graph $\controlgraph$
in which the control words are unreachable from the other words
except by paths that we construct.
Starting with graph $\norepgraph$, delete all incoming transitions to control words $\controlword_n \in \controlset$,
rendering them unreachable,
and prune the graph of any other nodes that become unreachable as a result.
Next, for every control word $\controlword_n \in \controlset$
and every path length $1 \leq k < \stepsto(\controlword_n,\controlset)$,
we create a new vertex set $\controlbridges{n,k}$
with a one-to-one correspondence to $\prequels(\controlword_n,\controlset,k)$.
We connect the newly-added vertices such that there is an edge from
$u \in \controlbridges{n,k}$ to $v \in \controlbridges{n,k-1}$
for every corresponding edge $(u',v') \in \norepedges$ between
$u' \in \prequels(\controlword_n,\controlset,k)$
and
$v' \in \prequels(\controlword_n,\controlset,k-1)$.
We also add edges from
$u \in \norepvertices$ to $v \in \controlbridges{n,\stepsto(\controlword_n,\controlset)}$
for the first steps in the paths to control words,
as well as edges from
$u \in \controlbridges{n,1}$ to $\controlword$
for the final steps.
In each case the newly-added edge is copied from a corresponding edge in $\norepedges$
and inherits the same edge label.

It can be useful to force the transducer to start with a particular control word $\initword$
and finish with another (or the same) control word $\finalword$.
To guarantee this we can add a chain of initial vertices and edges leading from source vertex to the initial control word,
$\initvertex{0} \to \initvertex{1} \to \initvertex{2} \to \ldots \to \initvertex{\kmerlen-1} \to \initword$,
with each transition labeled with consecutive symbols from the initial control word.
We also need to add a transition from the final control word to the final state.
(If the start and end control words are the same, then this vertex should be duplicated to prevent cycles.)

We have now constructed the graph $\controlgraph$ from which our transducer $\transcoder$
is derived via the following recipe:
\begin{itemize}
\item Every vertex in $\controlgraph$ is a state in $\transcoder$.
The initial and final vertices of $\controlgraph$ are the initial and final states of $\transcoder$.
\item Every edge in $\controlgraph$ is a unit-weight transition in $\transcoder$.
The output label of the transition is the label of the edge.
\item The input label of each transition is determined as follows:
\begin{itemize}
\item For states in $\norepvertices$ with two outgoing transitions to other states in $\norepvertices$, those transitions are input-labeled with the binary digits (bits) $\bit{0}$ and $\bit{1}$.
\item For states in $\norepvertices$ with three outgoing transitions to other states in $\norepvertices$, those transitions are input-labeled with the trinary digits (trits) $\trit{0}$, $\trit{1}$ and $\trit{2}$.
\item For states in $\norepvertices$ with four outgoing transitions to other states in $\norepvertices$, those transitions are input-labeled with the quaternary digits (quats) $\quat{0}$, $\quat{1}$, $\quat{2}$ and $\quat{3}$.
\item Transitions from states in $\norepvertices$ to states in $\controlbridges{n,\stepsto(\controlword_n,\controlset)}$,
which begin a path to the $n$'th control word $\controlword_n$,
are input-labeled with the special control digit $\controlsym{n}$.
\item All other transitions have input label $\emptystring$.
\end{itemize}
\end{itemize}

Thus, the input alphabet of $\transcoder$
is $\{ \bit{0}, \bit{1},
       \trit{0}, \trit{1}, \trit{2},
       \quat{0}, \quat{1}, \quat{2}, \quat{3},
       \controlsym{1} \ldots \controlsym{\ncontrols} \}$.

\figref{DNAStore} illustrates some of the code transducers that are generated by this procedure for the simplest case $\kmerlen=2$.
       
\subsection{A transducer with past and future context}

By virtue of being derived from the De Bruijn graph,
most of the states in the transducer $\transcoder$ of \secref{DeBruijnTransducer}
have $\kmerlen$ nucleotides of past output context.
We can transform this transducer into an equivalent one $\transdelay$ wherein most of the states have
$\kmerlen/2$ nucleotides of past output context
and $\kmerlen/2$ nucleotides of future output context.

This can be a convenient way to think about context, for the purpose of modeling errors in DNA replication and sequencing.
Common decoding and replication errors include local tandem and inverted duplications, as well as context-dependent substitutions.
These can occur on either strand of and therefore (depending on the representation) may be best represented as depending
on future context as well as past context.

The transformation requires that the output sequence of all successful paths through the transducer
begin with a particular $\kmerlen$-mer and end with a particular $\kmerlen$-mer,
which can be ensured using the method described in \secref{DeBruijnTransducer}.

Let $\seqpastsyms{1}{\kmerlen} = \seqpast$ be the past output context for a given state $\state$.
All of the transitions $\trans$ into $\state$ have the same input label $\inlab[\trans] = \sympast_{\kmerlen}$,
which corresponds to the most recent nucleotide of output context for that state.
If, instead, we set $\inlab[\trans]$ to be $\sympast_{\kmerlen/2}$, the $(\kmerlen/2)$'th symbol of $\someseq$
for all transitions into $\state$, then we have effectively delayed all output by $\kmerlen/2$ symbols.
We can now predict the next $\kmerlen/2$ symbols in the output sequence for the path from a state,
so we have traded $\kmerlen/2$ of past output context for $\kmerlen/2$ of future output context.

We need to add $\kmerlen/2$ extra padding states after (what was originally) the final state,
with a chain of transitions that flushes out the final $\kmerlen/2$ delayed output symbols.
Conversely, the first $\kmerlen/2$ transitions from the initial state
(into states with $\kmerlen/2$ or fewer nucleotides of output context)
will, after the transformation,
be null transitions (with both input and output labels equal to $\emptystring$),
so these transitions and the corresponding states can be deleted.

An analogous procedure can be used to transform a machine with $\kmerlen$ symbols of past input context
into a machine with $\kmerlen/2$ past input context and $\kmerlen/2$ future input context.

\subsection{A transducer that converts a binary sequence into a mixture of binary, trinary and quaternary}

Not every state accepts symbols of the same radix...

We need a converter that can context-switch between binary, trinary, and quaternary...

Consider first the simpler task of mapping a binary sequence to a trinary sequence...

To do this at the Shannon limit requires an arithmetic conversion of the binary number into the trinary number

As an side, somewhat reminiscent of arithmetic coding \cite{Mackay2003}
where a probability distribution over symbols is used to compress
a symbol stream into a floating-point binary number

The arithmetic conversion requires repeated divisons by three...

An automata-theoretic approach to this is illustrated in...
\figref{DivisionByThree}...

The 4-state machine $\transdivthree$ performs one division by three and outputs one trinary digit.
By composing $N$ of these together we can convert a binary number to $N$ trinary digits.
This takes $\sim 4^N$ states (some may be unreachable or duplicates that can be eliminated).

Converting a binary sequence to a mixed-radix sequence of bits, trits and quats
% can be done arithmetically in principle... explain the interpretation... radix at each position specified in advance
is a matter of taking the union of the division-by-three transducer
in \figref{DivisionByThree}
with the division-by-two and division-by-four transducers
in \figref{EvenDivision},
each of which divide a binary input by the appropriate divisor (2 or 4)
and output the modulus in the corresponding radix.
However, the union of these transducers has 12=3+4+5 states
and so the $N$-digit converter has $\sim 12^N$ states,
which is expensive.

We can improve on this e.g. by using machine state to record the input sequence
(the principle behind \subfigref{HammingTransducer}{b})
and hard-code the results of every division step.
However it is always $\bigo(A^N)$ for some $A$, except in the special cases where the input and output radix
are related by an integer multiplier, as in the base $2 \to 4$ conversion of \subfigref{NaiveDNATransducer}{a}.

In view of this, if we do not mind a small amount of inefficiency,
we can use a simpler mapping between a binary input and a mixed-radix output,
such as the mapping defined in \subfigref{Converter}{a}...

\subfigref{Converter}{a}
encodes 1 bit of input per bit of output,
2 bits of input per quat of output,
and 1.5 bits of input per trit of output.
For comparison,
an exact arithmetic conversion
encodes 1 bit of input per bit of output,
2 bits of input per quat of output,
and $\log_2(3) \simeq 1.58$ bits of input per trit of output.
This difference is small, is confined to trits only.
Indeed, if one is constrained to round up to the nearest integer number of output symbols per input byte,
then the two approaches are basically indistinguishable:
both will require 6 trits per byte, or 4 quats per byte, or 8 bits,
or some mixture of bits, trits and quats bounded by these amounts.

\subsection{Making the decoder robust to substitutions, local tandem and inverted duplications, and other indels}
\seclabel{ErrorModel}

In this section we describe a statistical error model,
implemented as a transducer $\transerror$,
that includes
tandem duplications (ACG$\to$ACG\underline{ACG}),
forward inverse duplications (ACG$\to$ACG\underline{CGT}),
reverse inverse duplications (ACG$\to$\underline{CGT}ACG),
and point substitutions (ACG$\to$A\underline{T}G).
The duplications can be imperfect: they can include substitutions.
We also describe how to account for partial observation of the sequence
(at least at the level of reconstructing individual reads;
the broader problem of reassembling a data file from fragments is not addressed here,
beyond the general recipe for demarcating metadata with control words
that was given in \secref{DeBruijnTransducer}, which can be used to mark up shorter blocks
with their locations in the file).

The full working is rather detailed, but the basic idea is very simple.
We use the transducer state space to build up a context of past nucleotides
(i.e. the last few nucleotides it has most recently seen on the input)
and future nucleotides
(i.e. the next few nucleotides it is prepared to receive on the input, in a given state).
The higher-order structure of the transducer is determined by how much past and future context it has built up
(in the early stages),
and how much future context it has yet to work through
(in the later stages).
The local structure of the transducer involves the states it uses to generate
deletions (with opening and extension probabilities)
and insertions (which are duplications that simply copy the past or future context,
depending on the type of duplication).

The transducer as constructed can only handle sequences whose length is at least $\kmerlen$.
Shorter sequences will have no successful path through the machine.
This is not envisaged to be a problem for decoding DNA-stored data
since reads of length $<\kmerlen$ will contain very little data,
probably insufficient for assembly and lacking in metadata
since $\kmerlen$ is the number of nucleotides required to encode a control signal.
Nevertheless, the transducer can readily be adapted for such edge cases
by adding extra blocks to the higher-order structure (see \subfigref{PartialObservation}{a}).

The error-model transducer is defined over the probabilistic semiring,
has input alphabet $\outalph$,
output alphabet $\outalph$,
past \& future context $\contextlen=\kmerlen/2$,
and the following state space:
\begin{itemize}
\item There is a state $\symstate{a}(\emptystring,\emptystring)$ which is the initial state
 %
\item For every $k:1 \leq k < \contextlen$ and every $k$-mer $\seqnext \in \outalph^k$
  there is a state $\symstate{b}(\emptystring,\seqnext)$
  %
\item For every $k:1 \leq k < \contextlen$, every $k$-mer $\seqpast \in \outalph^k$ and every $\contextlen$-mer $\seqnext \in \outalph^\contextlen$
  there are
  \begin{itemize}
  \item two states $\{ \symstate{c}(\seqpast,\seqnext),\ \delstate{c}(\seqpast,\seqnext) \}$
  \item $\contextlen$ states $\{ \revstate{i}{c}(\seqpast,\seqnext,i):\ 1 \leq i \leq \contextlen \}$
  \item $2k$ states $\{ \tanstate{c}{i}(\seqpast,\seqnext),\ \fwdstate{c}{i}(\seqpast,\seqnext): 1 \leq i \leq k \}$
  \end{itemize}
  %
\item For every pair of $\contextlen$-mers $\seqpast, \seqnext \in \outalph^\contextlen$
  there are
  \begin{itemize}
  \item two states $\{ \symstate{d}(\seqpast,\seqnext),\ \delstate{d}(\seqpast,\seqnext) \}$
  \item $3\contextlen$ states $\{ \tanstate{d}{i}(\seqpast,\seqnext),\ \fwdstate{d}{i}(\seqpast,\seqnext),\ \revstate{d}{i}(\seqpast,\seqnext):\ 1 \leq i \leq \contextlen \}$
  \end{itemize}
  %
\item For every $k:1 \leq k < \contextlen$, every $\contextlen$-mer $\seqpast \in \outalph^\contextlen$ and every $k$-mer $\seqnext \in \outalph^k$
  there are
  \begin{itemize}
  \item two states $\{ \symstate{e}(\seqpast,\seqnext),\ \delstate{e}(\seqpast,\seqnext) \}$
  \item $2\contextlen$ states $\{ \tanstate{e}{i}(\seqpast,\seqnext),\ \fwdstate{e}{i}(\seqpast,\seqnext):\ 1 \leq i \leq \contextlen \}$
  \item $k$ states $\{ \revstate{e}{i}(\seqpast,\seqnext):\ 1 \leq i \leq k \}$
  \end{itemize}
  %
\item There is a state $\symstate{f}(\emptystring,\emptystring)$ which is the final state
\end{itemize}
These states have the following significance (see \subfigref{PartialObservation}{a-b}:
\begin{itemize}
  \item $\symstate{b}$ states load input symbols into the future context queue
  \item $\symstate{c}$ states have fully loaded future context queues. They continue to load input symbols, but also start emitting output symbols and shifting input symbols to the past context queue
  \item $\symstate{d}$ states have fully loaded past and future context queues. They load input symbols, shift input symbols from future to past context queues, drop input symbols off the back of the past context queue, and emit output symbols
  \item $\symstate{e}$ states have fully loaded past context queues, but emptying future context queues. No future input symbols are loaded at this point. They shift input symbols from future to past context queues, drop input symbols off the back of the past context queue, and emit output symbols
  \item \subfigref{PartialObservation}{a} shows an additional block of $\symstate{g}$-states that would be required for the transducer to handle sequences of length $<2\contextlen$, which never make it to block \#d since they are too short to fully load the past and future context queues. Since these sequences probably contain too little information to be useful (especially if we are using $2\contextlen$-nucleotide words to mark up metadata), we have omitted them from the formal description.
  \item $\delstate{\block}$ states are used for deletions ($\block \in \{ c, d, e \}$)
  \item $\tanstate{\block}{i}$ states are used for tandem duplications ($1 \leq i \leq \contextlen$)
  \item $\fwdstate{\block}{i}$ states are used for forward inverse duplications
  \item $\revstate{\block}{i}$ states are used for reverse inverse duplications
  \item Each state is indexed with its past context $\seqpast$, its future context $\seqnext$ and (for duplication states) the remaining duplication length $i$
\end{itemize}

The transitions $(p, \inlab, \outlab, \weight, q)$,
that involve $S$-states,
so $p = \symstate{\srcblock}(\ldots)$ and $q = \symstate{\destblock}(\ldots)$,
are shown in the table.
In all cases
$1 < k \leq \contextlen$ measures a partial context length,
$\seqpastsyms{1}{\contextlen} \in \outalph$ represent past input context symbols,
$\seqnextsyms{1}{\contextlen} \in \outalph$ represent future input context symbols and
$\outsym \in \outalph$ represents an output symbol.
\[
\begin{array}{lllll}
\src & \inlab & \outlab & \weight & \dest \\
\hline
\symstate{a}(\emptystring,\ \emptystring) & \symnext_1 & \emptystring & 1 & \symstate{b}(\emptystring,\ \symnext_1) \\
\symstate{b}(\emptystring,\ \seqnextsyms{1}{k-1}) & \symnext_k & \emptystring & 1 & \symstate{b}(\emptystring,\ \seqnextsyms{1}{k}) \\
\symstate{b}(\emptystring,\ \symnext_1\seqnextsyms{2}{\contextlen}) & \symnext_{\contextlen+1} & \outsym & \pcont(0,\contextlen) \cdot \psubnext{1} & \symstate{c}(\symnext_1,\ \seqnextsyms{2}{\contextlen}\symnext_{\contextlen+1}) \\
\symstate{c}(\seqpastsyms{1}{k-1},\ \symnext_1\seqnextsyms{2}{\contextlen}) & \symnext_{\contextlen+1} & \outsym & \pcont(k-1,\contextlen) \cdot \psubnext{1} & \symstate{c}(\seqpastsyms{1}{k-1}\symnext_1,\ \seqnextsyms{2}{\contextlen}\symnext_{\contextlen+1}) \\
\symstate{c}(\seqpastsyms{1}{\contextlen-1},\ \symnext_1\seqnextsyms{2}{\contextlen}) & \symnext_{\contextlen+1} & \outsym & \pcont(\contextlen-1,\contextlen) \cdot \psubnext{1} & \symstate{d}(\seqpastsyms{1}{\contextlen-1}\symnext_1,\ \seqnextsyms{2}{\contextlen}\symnext_{\contextlen+1}) \\
\symstate{d}(\sympast_1\seqpastsyms{2}{\contextlen},\ \symnext_1\seqnextsyms{2}{\contextlen}) & \symnext_{\contextlen+1} & \outsym & \pcont(\contextlen,\contextlen) \cdot \psubnext{1} & \symstate{d}(\seqpastsyms{2}{\contextlen}\symnext_1,\ \seqnextsyms{2}{\contextlen}\symnext_{\contextlen+1}) \\
\symstate{d}(\sympast_1\seqpastsyms{2}{\contextlen},\ \symnext_1\seqnextsyms{2}{\contextlen}) & \emptystring & \outsym & \pcont(\contextlen,\contextlen) \cdot \psubnext{1} & \symstate{e}(\seqpastsyms{2}{\contextlen}\symnext_1,\ \seqnextsyms{2}{\contextlen}\symnext_{\contextlen+1}) \\
\symstate{e}(\sympast_1\seqpastsyms{2}{\contextlen},\ \symnext_1\seqnextsyms{2}{k}) & \emptystring & \outsym & \pcont(\contextlen,k) \cdot \psubnext{1} & \symstate{e}(\seqpastsyms{2}{\contextlen}\symnext_1,\ \seqnextsyms{2}{k-1}) \\
\symstate{e}(\seqpastsyms{1}{\contextlen},\ \symnext_1) & \emptystring & \outsym & \pcont(\contextlen,1) \cdot \psubnext{1} & \symstate{f}(\emptystring,\ \emptystring) \\
\hline
\end{array}
\]

The other transitions can be deduced using the following rules:
\[
\begin{array}{llllll}
  \multicolumn{6}{l}{
    \text{For every state of the form...}
  } \\
\hline
\symstate{\block}(\seqpast,\seqnext) & & & & & \text{with $\seqpast = \seqpastsyms{1}{j},\ 0 \leq j \leq \contextlen$} \\
& & & & & \text{and $\seqnext = \seqnextsyms{1}{k},\ 0 \leq k \leq \contextlen$} \\
\hline
\\
%
\multicolumn{6}{l}{
  \text{...there are transitions of the form...}
} \\
\src & \inlab & \outlab & \weight & \dest \\
\hline
\symstate{\block}(\seqpast,\seqnext) & \emptystring & \emptystring & \ptandup \lendist(i) & \tanstate{\block}{i}(\seqpast,\seqnext) & \forall 1 \leq i \leq j \\
\symstate{\block}(\seqpast,\seqnext) & \emptystring & \emptystring & \pfwddup             & \fwdstate{\block}{1}(\seqpast,\seqnext) & \\
\symstate{\block}(\seqpast,\seqnext) & \emptystring & \emptystring & \prevdup \lendist(i) & \revstate{\block}{i}(\seqpast,\seqnext) & \forall 1 \leq i \leq k \\
%
\tanstate{\block}{i}(\seqpast,\seqnext) & \emptystring & \outsym & \psubpast{j+1-i} & \tanstate{\block}{i-1}(\seqpast,\seqnext) & \forall 1 < i \leq j \\
\tanstate{\block}{1}(\seqpast,\seqnext) & \emptystring & \outsym & \psubpast{j} & \symstate{\block}(\seqpast,\seqnext) & \\
%
\fwdstate{\block}{i}(\seqpast,\seqnext) & \emptystring & \outsym & \psubcomppast{j+1-i} & \fwdstate{\block}{i+1}(\seqpast,\seqnext) & \forall 1 \leq i < j \\
\fwdstate{\block}{1}(\seqpast,\seqnext) & \emptystring & \outsym & \lendist(i) \psubcomppast{j+1-i} & \symstate{\block}(\seqpast,\seqnext) & \forall 1 \leq i \leq j \\
%
\revstate{\block}{i}(\seqpast,\seqnext) & \emptystring & \outsym & \psubcompnext{i} & \revstate{\block}{i-1}(\seqpast,\seqnext) & \forall 1 < i \leq k \\
\revstate{\block}{1}(\seqpast,\seqnext) & \emptystring & \outsym & \psubcompnext{1} & \symstate{\block}(\seqpast,\seqnext) & \\
\hline
%
\\
\multicolumn{6}{l}{
    \text{For every transition of the form...}
  } \\
\src & \inlab & \outlab & \weight & \dest \\
\hline
%
\symstate{\srcblock}(\seqpast,\seqnext) & \sym & \outsym & \pcont(\ldots) \psub & \symstate{\destblock}(\seqpast',\seqnext') & \text{with $\seqpast,\seqpast',\seqnext,\seqnext' \in \kleene{\outalph}$} \\
  & & & & & \text{and $\sym,\outsym \in \outalph$} \\
\hline
%
\\
  \multicolumn{6}{l}{
    \text{...there are also transitions of the form...}
  } \\
\src & \inlab & \outlab & \weight & \dest \\
\hline
\symstate{\srcblock}(\seqpast,\seqnext) & \sym & \emptystring & \pdelopen & \delstate{\destblock}(\seqpast',\seqnext') & \text{if $\destblock \neq f$} \\
\symstate{\srcblock}(\seqpast,\seqnext) & \sym & \emptystring & \pdelopen & \symstate{f}(\emptystring,\emptystring) & \text{if $\destblock = f$} \\
\delstate{\srcblock}(\seqpast,\seqnext) & \sym & \emptystring & \pdelext & \delstate{\destblock}(\seqpast',\seqnext') & \text{if $\srcblock \neq b$} \\
\delstate{\srcblock}(\seqpast,\seqnext) & \emptystring & \emptystring & \pdelend & \symstate{\srcblock}(\seqpast',\seqnext') & \text{if $\srcblock \neq b$} \\
\hline
\end{array}
\]

The probability parameters are
$\pdelopen$ to open a deletion,
$\pdelext$ to extend a deletion,
$\pdelend$ to end a deletion,
$\ptandup$ for a tandem duplication,
$\pfwddup$ for a forward inverse duplication,
$\prevdup$ for a reverse inverse duplication,
$\pnogap$ for no gap,
$\ptransition$ for a transition substitution,
$\ptransversion$ for a transversion substitution,
$\pmatch$ for no substitution, and
$\plen{k}$ for the probability that a duplication has length $k$.
The constraints on these parameters are
$\pdelopen+\ptandup+\pfwddup+\prevdup+\pnogap=1$,
$\pdelext+\pdelend=1$,
$\ptransition+\ptransversion+\pmatch=1$,
and
$\sum_{k=1}^{\kmerlen} \plen{k} = 1$.

The substitution matrix is $\submat(\sym,\outsym)$, defined to be $\pmatch$ if $x=y$ (no substitution), $\ptransition$ if $x = \ntrans{y}$ (transition)
and $\ptransversion/2$ otherwise (transversion).

The context-adjusted probability of not opening a gap at a site with $j$ nucleotides of past context and $k$ nucleotides of future context is
\[
\pcont(j,k) = \pnogap + (\ptandup + \pfwddup) \sum_{i=j+1}^{\kmerlen} \lendist(i) + \prevdup \sum_{i=k+1}^{\kmerlen} \lendist(i)
\]

The probability parameters can be estimated directly from alignments of reads to known reference sequences using the Forward-Backward algorithm \cite{Durbin98}.

The transducer $\transerror$
described in this section models errors but still assumes observation of the full-length sequence.
By composing it with the transducer in \subfigref{PartialObservation}{c}, we can model errors and partial observation.

\subsection{Adding error-correcting codes}

Hamming codes \figref{HammingTransducer}, convolutional codes % need reference

Many such codes can be conveniently represented as state machines

\section{Results}

DNAStore

\section{Discussion}


We do not address the higher-level organization of DNA ``filesystem'',
which has been addressed by others \cite{YazdiEtAl2015}
The code described here operates at a lower level, though our use of reserved ``control words''
can be used to embed in-band metadata that may be useful in organizing files into manageably-sized blocks
(for example, oligonucleotide sequences)


\newpage
\section{Figures}

\newpage
\begin{figure}
\begin{tabular}{ll}
(a) \includedot{hamming31}{width=.45\textwidth}
&
(b) \includegraphics[width=.45\textwidth]{hamming74.ps}
\end{tabular}
\caption{ \figlabel{HammingTransducer}
State machines implementing Hamming codes for error correction.
  Left:
Transducer $\hammingthreeone$ implements the Hamming(3,1) error-correcting code
(which simply repeats every bit three times).
S is both the initial state and the final state.
Right:
Transducer $\hammingsevenfour$ implements the Hamming(7,4) error-correcting code,
with four data bits and three parity bits.
Due to the large number of states, the state names have been omitted from this diagram,
as have the $\epsilon$ labels for empty inputs or outputs.
}
\end{figure}

\newpage
\begin{figure}
\begin{tabular}{lll}
(a) \includedot{binary2quaternary}{width=.3\textwidth}
&
(b) \includedot{quaternary2dna}{width=.3\textwidth}
&
(c) \includedot{binary2dna}{width=.3\textwidth}
\end{tabular}
\caption{
\figlabel{NaiveDNATransducer}
State machines implementing naive conversions of binary sequences to DNA.
Left:
Transducer $\transbintoquat$ converts a binary input string (presented least-significant digit first) into a quaternary input string.
Binary input digits are written showing the radix-2 suffix ($\bit{0}$, $\bit{1}$),
while quaternary output digits are written showing the radix-4 suffix, ($\quat{0}$, $\quat{1}$, $\quat{2}$, $\quat{3}$).
The initial state is S and the final state is E.
Middle:
Transducer $\transquattodna$ converts a quaternary input string into a DNA string.
Right:
Transducer $\transbintodna$
converts a binary input string to DNA.
This machine can be regarded as a composition of the ``binary to quaternary'' transducer ($\transbintoquat$) with the ``quaternary to DNA'' transducer ($\transquattodna$),
and so can be written $\transbintodna = \transbintoquat \transcomp \transquattodna$.
}
\end{figure}

\newpage
\begin{figure}
\begin{tabular}{cc}
\multicolumn{2}{c}{
(a) \includedot{divisionby3}{width=\textwidth}
}
\\
(b) \includedot{trinary0}{width=.2\textwidth}
&
(c) \includedot{trinaryunion}{width=.2\textwidth}
\\
(d) \includedot{trinaryid}{width=.2\textwidth}
&
(e) \includedot{binaryeraser}{width=.15\textwidth}
\\
\multicolumn{2}{c}{
  (f) \includedot{binary2trinary}{width=\textwidth}
  }
\end{tabular}
\caption{
\figlabel{DivisionByThree}
An automata-theoretic approach to long division by the number three,
relevant to conversion from binary to trinary.
Top row (a):
Transducer $\transdivthree$ implements the operation ``division by three''.
The machine accepts as its input sequence a binary number representing the dividend, which must be presented most significant bit first.
It outputs the quotient (as a similar binary sequence) followed by the remainder (as a trinary digit).
The machine's state encodes the current remainder at any stage during the division.
Binary digits are written as $(\bit{0}, \bit{1})$
and trinary digits as $(\trit{0}, \trit{1}, \trit{2})$.
Second row (b,c):
Transducers that are useful in implementing automata-theoretic division, also illustrating the operation of transducer union.
Left of second row:
Transducer $\transternzeroid$ echoes a single zero trit ($\trit{0}$) from input to output.
Adopting the notation $\transsingle{\inlab}{\outlab}$ for a transducer with one transition $(S,\inlab,\outlab,1,T)$
we can write $\transternzeroid = \transid{\trit{0}}$.
Right of second row:
Transducer $\transternsymid$ echoes any single trit ($\trit{0}$, $\trit{1}$ or $\trit{2}$).
This machine may be viewed as the union of three single-transition transducers,
$\transternsymid = (\transid{\trit{0}}) \cup (\transid{\trit{1}}) \cup (\transid{\trit{2}})$.
Third row (d,e):
More transducers that are useful for automata-theoretic division, also illustrating Kleene closure.
Left of third row:
Transducer $\transternseqid$ echoes any sequence of trits, and may be written as
the Kleene closure $\transternseqid = \kleene{(\transternsymid)}$.
Right of third row:
Transducer $\transerasezerobits$ erases any sequence of binary zeroes,
while passing through any symbols that are not binary digits.
The transition labeled $!2$ is a shorthand for the set of transitions $y/y$
with $y \notin \{ \bit{0}, \bit{1} \}$
that echo anything which is {\em not} a binary digit.
Bottom row:
Transducer $\transechodivthree$,
when fed a binary sequence $X$ representing a dividend,
followed by a trinary sequence $Y$ representing the remainders from previous divisions,
will print out the quotient $X/3$ (in binary), followed by $Y$, followed by the new remainder digit $X \mod 3$ (in trinary).
By composing a series of $N$ transducers of this form, along with a final step that removes the binary zeroes,
we can convert a binary number into $N$ trinary digits \cite{MartinsFerreira2012}.
For example, to output three trinary digits, we could use the composed transducer
$(\transdivthree) \transcomp (\transechodivthree) \transcomp (\transechodivthree) \transcomp (\transerasezerobits)$.
}
\end{figure}

\newpage
\begin{figure}
\begin{tabular}{ll}
  (a) \includedot{divisionby2}{width=.5\textwidth}
&
  (b) \includedot{divisionby4}{width=.5\textwidth}
  \end{tabular}
\caption{
\figlabel{EvenDivision}
An automata-theoretic approach to long division by two (a) and by four (b),
relevant to conversion from binary to mixed-radix sequences
that include binary and quaternary digits.
Left:
Transducer $\transechodivtwo$ models a single step of long division by two.
When fed a binary sequence $X$ representing a dividend,
followed by a sequence $Y$ representing any remainders from previous divisions,
will print out the quotient $X/2$ (in binary), followed by $Y$, followed by the new remainder digit $X \mod 2$ (in binary).
To distinguish quotient from remainder,
the remainder is expressed using a different alphabet $\{ \rbit{0}, \rbit{1} \}$.
The quotient is expressed using the same alphabet as the dividend $\{ \bit{0}, \bit{1} \}$.
The transition labeled $!2$ is a shorthand for the set of transitions $y/y$
with $y \notin \{ \bit{0}, \bit{1} \}$
that echo anything which is {\em not} a binary digit;
these will pass remainder digits through unaffected.
Right:
Transducer $\transechodivfour$ models a single step of long division by four.
When fed a binary sequence $X$ representing a dividend,
followed by a sequence $Y$ representing any remainders from previous divisions,
it will print out the quotient $X/4$ (in binary), followed by $Y$, followed by the new remainder digit $X \mod 2$ (in quaternary).
We can construct a single-step mixed-radix long-division transducer capable of dividing by two, three or four
using transducer union $\transechodivmixed = (\transechodivtwo \cup \transechodivthree \cup \transechodivfour)$,
and then construct a transducer that can encode a binary input into a sequence of up to $N$ such mixed-radix digits
via a composition of $N$ such transducers.
For example, to convert three mixed-radix digits:
$(\transechodivmixed) \transcomp (\transechodivmixed) \transcomp (\transechodivmixed) \transcomp (\transerasezerobits)$.
}
\end{figure}

\newpage
\begin{figure}
\begin{tabular}{ll}
  (a) \includedot{converter}{width=.5\textwidth}
&
  (b) \includedot{flush8}{width=.5\textwidth}
  \end{tabular}
\caption{
\figlabel{Converter}
An inefficient, but simple, approach
to the conversion of a binary input into a mixed-radix output (binary, trinary and quaternary).
Left:
Transducer $\transmixed$ converts an input sequence of binary digits ($\bit{0},\bit{1}$),
optionally interleaved with
control symbols ($\controlsym{n}$)
and flush symbols ($\flushsym$),
into a mixed output of binary digits,
trinary digits ($\trit{0},\trit{1},\trit{2}$),
quaternary digits ($\quat{0},\quat{1},\quat{2},\quat{3}$),
and control symbols, which are simply echoed to the output;
the flush symbol is used to drive the machine into a state (T) where it can accept control symbols.
The conversion is suboptimal in that trinary digits (trits) on average encode
1.5 input bits, slightly less than the theoretical maximum of $\log_2(3) \simeq 1.58$ bits.
State S is both the initial state and the final state.
Right:
Transducer $\transflush$ copies input bits to the output, sending a flush symbol after every 8-bit byte.
Accurate decoding of the output sequence from $\transmixed$ requires that the decoder
knows at what points in the sequence flush symbols have been encoded.
This synchronization can be formalized by using $\transflush \transcomp \transmixed$,
i.e. passing the input sequence through the ``flush every 8 bits'' transducer
before feeding it to the ``convert to binary/trinary/quaternary'' transducer.
In this model, bytes cannot be split up; a control symbol can only be sent before or after a byte,
and an input bitstream that has a control symbol partway through an 8-bit block
will have no successful path through the transducer.
}
\end{figure}

\newpage
\begin{figure}
\begin{tabular}{ccc}
(a) \includedot{dna2full}{width=.3\textwidth}
&
(b) \includedot{dna2start}{width=.3\textwidth}
&
(c) \includedot{dna2startend}{width=.3\textwidth}
\\
(d) \includedot{dna2norep}{width=.3\textwidth}
&
(e) \includedot{dna2startnorep}{width=.3\textwidth}
&
(f) \includedot{dna2startendnorep}{width=.4\textwidth}
\end{tabular}
\caption{
  \figlabel{DNAStore}
  Transducers generated using the method of \secref{DeBruijnTransducer} with $\kmerlen=2$.
  These codes are all fundamentally based on the 2-dimensional De Bruijn graph
  from which vertices are duplicated, deleted and added to arrive at a transition graph with the required properties.
  Top row (a,b,c): codes in which dinucleotide repeats are not prohibited.
  Bottom row (d,e,f): codes in which dinucleotide repeats are not prohibited.
  Left column (a,d): codes in which there are no reserved control words, so the machine start and end in arbitrary states.
  Central column (b,e): codes in which there is one reserved control word, which only ever appears once, at the start of the encoded DNA sequence.
  Right-hand column (c,f): codes in which there is one reserved control word, which only ever appears twice: once at the start of the encoded DNA sequence and once at the end.
  The leftmost machine in the top row (a) is similar to $\transbintodna$ of \subfigref{NaiveDNATransducer}{c},
  while the leftmost machine in the bottom row (d) is similar to the trinary code of \cite{GoldmanEtAl2013}.
  {\bf Key:}
  Transition label annotations have been omitted from this diagram.
  Instead, the labels may be deduced from the node and edge shapes, as follows:
  Bold-line transitions from rectangular states encode quaternary input digits.
  Solid-line transitions from triangular states encode trinary input digits.
  Dashed-line transitions from double-circle states encode binary input digits.
  Dotted-line transitions do not encode input digits;
  states that can only be exited via these transitions are shown as rectangles.
  Transitions that encode input digits have empty circles at the source end;
  transitions that encode output digits have filled arrowheads at the destination end.
  The output label of a transition into a state $XY$ is always either $Y$ or $\epsilon$.
}
\end{figure}

\begin{figure}
\begin{tabular}{c}
(a) \includedot{blocks}{width=.9\textwidth}
\\
(b) \includedot{error}{width=.9\textwidth}
\\
(c) \includedot{partial}{width=.9\textwidth}
\end{tabular}
\caption{
  \figlabel{PartialObservation}
  Models of error and partial observation (\secref{ErrorModel}).
  Top: Higher-order structure of the error model, showing blocks \#a through \#f.
  The dashed lines show the extra block (\#g) and transitions that would need to be added to handle
  sequences shorter than the total (past+future) context length,
  which require paths that bypass the full loading of both context queues (block \#d).
  Middle: Local neighborhood of a state in the error model for context length $\contextlen=3$.
  For simplicity, the diagram includes only transitions within block \#d, and
  only transitions to or from states with a given context (representing the ``current'' input symbol).
  The $D$ states model deletions;
  the $S$ states model substitutions;
  $T1,T2,T3$ model tandem duplications
  (gieven past context $ACG$, insert $ACG$, yielding the observed mutation $ACG \to ACGACG$);
  $F1,F2,F3$ model forward inverted duplications
  (given past context $ACG$, insert $CGT$, yielding the observed mutation $ACG \to ACGCGT$);
  and
  $R1,R2,R3$ model reverse inverted duplications
  (given future context $ACG$, insert $CGT$, yielding the observed mutation $ACG \to CGTACG$).
  The probabilities and length distributions of these various events can be modeled.
  Bottom: Transducer $\transpartial$ models partial observation of a sequence.
  Borrowing the terminology of local vs global alignment \cite{Durbin98},
  this machine effectively converts a global model for a sequence into a local one.
}
\end{figure}

\newpage
\bibliography{trans}



\end{document}
